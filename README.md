# ğŸ¤– AI Smart Recruiter

> A local AI-powered hiring assistant that matches resumes to job descriptions using Vector Embeddings and Llama 3.

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.31.0-FF4B4B)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

## âœ¨ Highlights

- âš¡ **50-80% faster** with intelligent model caching
- ğŸ”’ **100% local** - No data sent to cloud
- ğŸ¯ **Smart matching** using semantic similarity
- ğŸ¤– **AI explanations** powered by Llama 3
- ğŸ“ **Professional logging** for debugging

## ğŸš€ Quick Start

```bash
# 1. Clone and install
git clone <your-repo>
cd llm-project_recruter_ai
pip install -r requirements.txt

# 2. Start Ollama (if using AI explanations)
ollama pull llama3

# 3. Run the app
streamlit run app.py
```

## ğŸ“‚ Project Structure

```
llm project_recruter_ai/
â”œâ”€â”€ ğŸ“± app.py                    # Main Streamlit application
â”œâ”€â”€ âš™ï¸ config.py                 # Configuration management
â”œâ”€â”€ ğŸ“ logger.py                 # Logging system
â”œâ”€â”€ ğŸ“¦ requirements.txt          # Dependencies (pinned)
â”‚
â”œâ”€â”€ ğŸ“‚ resume_matcher/           # Core business logic
â”‚   â”œâ”€â”€ resume_parser.py         # PDF text extraction
â”‚   â”œâ”€â”€ matcher.py               # Semantic matching (cached)
â”‚   â””â”€â”€ explainer.py             # AI explanations
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                     # ğŸ“š Documentation
â”‚   â”œâ”€â”€ CHANGELOG.md             # Version history
â”‚   â””â”€â”€ IMPROVEMENTS_SUMMARY.md  # Detailed improvements
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                  # ğŸ› ï¸ Utility scripts
â”‚   â”œâ”€â”€ test_improvements.py    # Automated tests
â”‚   â””â”€â”€ run_test.py              # Test runner
â”‚
â”œâ”€â”€ ğŸ“‚ data/                     # Sample resumes
â”œâ”€â”€ ğŸ“‚ uploads/                  # Temporary uploads
â””â”€â”€ ğŸ“‚ logs/                     # Application logs
```

## âš™ï¸ Configuration

Create a `.env` file (optional):

```bash
# Copy template
cp .env.example .env

# Edit with your values
MODEL_NAME=all-MiniLM-L6-v2
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3
MAX_FILE_SIZE_MB=10
```

Or edit `config.py` directly.

## ğŸ¯ Features

| Feature | Description |
|---------|-------------|
| **Smart Matching** | Semantic similarity using sentence-transformers |
| **PDF Parsing** | Automatic text extraction from resumes |
| **Visual Ranking** | Color-coded scores and rankings |
| **AI Explanations** | Llama 3 explains why candidates match |
| **Model Caching** | 50-80% faster after first load |
| **Logging** | Structured logs in `logs/` directory |

## ğŸ“Š Performance

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| First analysis | 5-7s | ~5s | Baseline |
| Subsequent | 5-7s | <1s | **~80% faster** |
| Memory | Variable | Optimized | Shared model |

## ğŸ§ª Testing

```bash
# Run automated tests
python scripts/test_improvements.py

# View logs
cat logs/app_$(date +%Y%m%d).log
```

## ğŸ“š Documentation

- [ğŸ“– Full Documentation](docs/)
- [ğŸ“‹ Changelog](docs/CHANGELOG.md)
- [ğŸ“Š Improvements Summary](docs/IMPROVEMENTS_SUMMARY.md)

## ğŸ› ï¸ Requirements

- Python 3.8+
- Ollama (optional, for AI explanations)
- ~500MB disk space (for model cache)

## ğŸ¤ Contributing

Contributions welcome! Please check the documentation for guidelines.

## ğŸ“ License

MIT License - see LICENSE file for details.

## ğŸ’¬ Support

For issues or questions, please open an issue on GitHub.

---

**Made with â¤ï¸ using Python, Streamlit, and AI**
